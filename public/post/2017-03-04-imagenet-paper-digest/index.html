<!DOCTYPE html>
<html lang="ru">
	<head>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta name="generator" content="Hugo 0.70.0-DEV" />
		<title>Заметки о статьях по DeepLearning и ImageNet - Re9ulus Blog</title>

		<meta name="description" content="Краткая заметка о содержании классических статей по сверточным нейронным сетям и ImageNet.">


		
	
		




<link rel="stylesheet" href="/re9ulus.github.io/css/ui.css">

	
		

		<link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono|Lato|Raleway">

		
	</head>

<body>
<header class="container no-print">
	<div class="u-header">
		<nav class="bar">
	<ul><li>
			<a href="/">
				<img class="icon-text" src="/img/prev.svg"/>
			</a>
		</li><li><a href="re9ulus.github.io/about">About</a></li><li><a href="re9ulus.github.io/post/">Posts</a></li></ul>
</nav>

	</div>
</header>
<main class="container">

<article>
	<header><hgroup id="brand">
	<h1>Заметки о статьях по DeepLearning и ImageNet</h1>
	<h5>
		
		<time datetime="2017-03-04 00:00:00 &#43;0000 UTC">Mar 04, 2017</time>
		<span class="no-print">
			<span>
	</h5>
	
</hgroup>
<hr class="sep" />
</header>
	<p>Краткая заметка о содержании классических статей по сверточным нейронным сетям и <em>ImageNet</em>.</p>
<h2 id="imagenet-classification-with-deep-convolutional-neural-networks">ImageNet classification with deep convolutional neural networks</h2>
<p><!-- raw HTML omitted -->Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &ldquo;Imagenet classification with deep convolutional neural networks.&rdquo;
Advances in neural information processing systems. 2012. [<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">pdf</a>] (AlexNet, Deep Learning Breakthrough)<!-- raw HTML omitted --></p>
<p>Одна из классических статей, с которых начался современный виток диплернинга (сеть <em>AlexNet</em>).</p>
<h3 id="про-imagenet">Про ImageNet</h3>
<p>Появление датасета <a href="http://www.image-net.org">ImageNet</a> позволило решать задачу классификации изображений на качественно новом уровне. <em>ImageNet</em> содержит более 15 миллионов размеченных изображений, разбитых на 22000 категорий.</p>
<p>Начиная с 2010 года проводится соревнование <em>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</em> по распознаванию и детекции на <em>ImageNet</em>.
Датасет соревнования представлен 1000 классов и разделен на 3 части: train 1.3M, validation 50k, test 100k. Качество классификации оценивается по 2 метрикам: ошибка на топ 1 и ошибка на топ 5.</p>
<h3 id="архитектура-сети">Архитектура сети</h3>
<p>Авторы использовали сверточную сеть из 8 слоев: 5 сверточных, 3 полносвязных. Последний слой — <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> на 1000 классов.</p>
<p><img src="/img/imagenet/alexNet.png" alt="AlexNet network">
Оригинальная схема из статьи. (Да, она неправильно обрезана в статье.)</p>
<p>А еще в схеме есть забавная <a href="https://youtu.be/LxfUGhug-iQ?t=48m58s">неточность</a>.</p>
<p>Для тренировки сети все изображения масштабировали в 256x256. Препроцессинг — вычитание среднего (<em>mean</em>).</p>
<p>Функцией активации вместо общепринятых тогда <a href="https://en.wikipedia.org/wiki/Sigmoid_function">сигмоида</a> и <a href="http://functions.wolfram.com/ElementaryFunctions/Tanh/introductions/Tanh/ShowAll.html">tanh</a> выбрали <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>, что в несколько раз увеличило скорость обучения.</p>
<p>Хотя ReLU не требует нормализации данных, после экспериментов, авторы решили использовать локальную нормализацию (local normalization). Хитрая формула нормализации приведена в статье.</p>
<h3 id="борьба-с-оверфитом">Борьба с оверфитом</h3>
<p>Использовали 2 основных подхода: <a href="http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html">аугментации</a> и <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout">дропаут</a>.</p>
<p><strong>Аугментации</strong>. Аугментированные изображения генерировали на лету с помощью Python и не хранили на диске. Пока сеть перемножала тензоры на GPU, через CPU обсчитывали новое аугментированное изображение.</p>
<p>Использовали 2 вида аугментаций:</p>
<ul>
<li>извлечение случайных фрагментов изображений 224x224 из изображения 256x256 и горизонтальное отображение этих фрагментов.</li>
<li>изменение <em>RGB</em> компонет. Из <em>imageNet</em> с помощью <em>PCA</em> получили главные компоненты, которые хитрым способом сложили с оригинальными изображениями. Это уменьшило ошибку на тесте более чем на 1%.</li>
</ul>
<p>Второй подход: <strong>дропаут</strong> с коэффициентом 0.5 в первых двух полносвязных слоях. На каждой итерации случайным образом половина нейронов не учавствовала в обучении, что улучшило обобщаущую способность сети.</p>
<h3 id="детали-обучения">Детали обучения</h3>
<p>Модель обучали стохастическим градиентным спуском с импульсом (<em>momentum</em>) и затуханием (<em>weight decay</em>). Веса иницилизировали Гауссовым распределением с центром в 0 и среднеквадратическим отклонением 0.01.</p>
<p>Обучали на двух GPU GTX 580 (по 3Gb памяти) в течении 6 дней.</p>
<p>Полученные результаты:</p>
<ul>
<li>ошибка на топ 1: 35.7%</li>
<li>ошибка на топ 5: 17.0%</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="very-deep-convolutional-networks-for-large-scale-image-recognition">Very deep convolutional networks for large-scale image recognition</h2>
<p><!-- raw HTML omitted -->Simonyan, Karen, and Andrew Zisserman. &ldquo;Very deep convolutional networks for large-scale image recognition.&rdquo;
arXiv preprint arXiv:1409.1556 (2014). [<a href="https://arxiv.org/pdf/1409.1556.pdf">pdf</a>] (VGGNet,Neural Networks become very deep!)<!-- raw HTML omitted --></p>
<p>Вторая классическая статья по сверточным сетям. Этой статье мы обязаны архитектурами <a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8">VGG-16</a> и <a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77">VGG-19</a>.</p>
<h3 id="архитектура">Архитектура</h3>
<p>Сеть организована по тем же принципам что в вышеприведенной статье. Для обучения использованы 224x224 <em>RGB</em> изображения, препроцессинг — вычитаение среднего (<em>mean</em>).</p>
<p>Ключевая особенность полученных сетей — использование маленьких сверток 3x3, что ползволило увеличить глубину до 16-19 слоев.</p>
<p>Ранее использовались свертки 5x5 и 7x7. Согласно статье использование двух сверточных слоев 3x3 заменяет один слой 5x5, а использование 3 слоев 3x3 заменяет один слой 7x7. Замена одной большой свертки на несколько маленьких позволяет увеличить число нелинейных преобразований и значительно уменьшить количество параметров сети.</p>
<p>После сверточных слоев идут два полносвязных слоя на 4096 нейронов и последний <em>softmax</em> слой на 1000. В качестве функции активации примененили <em>ReLU</em>.</p>
<p>От использования локальной нормализации из вышеприведенной статьи отказались, т.к. на тестах она не дала никаких преимуществ, зато увеличила потребление памяти.</p>
<p>Сеть обучали на оптимизацию множественной логистической регрессии, используя градиентный спуск с импульсом (<em>momentum</em>), регуляризацию L2 и дропаут на первых двух полносвязных слоях.</p>
<p>Веса инициализировали используя предварительное обучение на сети с меньшим количеством слоев и случайно инициализированными параметрами.</p>
<h3 id="реализация">Реализация</h3>
<p>Сеть собрали на <a href="http://caffe.berkeleyvision.org">Caffe</a>, модифицировав для возможности обучения на нескольких GPU. Тренировка занимала 2-3 недели на 4х NVIDIA Titan Black.</p>
<p>Лучшие результаты на 1 сети:</p>
<ul>
<li>ошибка на топ 1: 24.4%</li>
<li>ошибка на топ 5: 7.1%</li>
</ul>
<p>Усреднение <em>softmax</em> классов нескольких сетей улучшило результаты:</p>
<ul>
<li>ошибка на топ 1: 23.7%</li>
<li>ошибка на топ 5: 6.8%</li>
</ul>
<p>Затем эту сеть применили к задаче локализации (определение баундинг-боксов объектов). Обучали аналогично задаче классификации, только логистическую регрессию заменили на Евклидово расстояние. Полученные результаты превзошли state of the art на момент написания статьи.</p>
<p>Сеть обученную на <em>ImageNet</em> применили к другим датасетам. Для этого выкинули последний полносвязный слой. Предпоследний слой, содержащий 4096 нейронов использовали в качестве извлекаемых из изображения признаков. К полученным признаками применили <em>SVM</em> классификатор с L2 регуляризацией. Веса предобученных слоев не меняли. На новых данных были получены результаты аналогичные либо превосходящие state of the art.</p>
<!-- raw HTML omitted -->
<h2 id="going-deeper-with-convolutions">Going Deeper with Convolutions</h2>
<p><!-- raw HTML omitted -->Szegedy, Christian, et al. &ldquo;Going deeper with convolutions.&rdquo;
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">pdf</a>] (GoogLeNet)<!-- raw HTML omitted --></p>
<p>Статья про <em>GoogleLeNet</em>.</p>
<h3 id="архитектура-1">Архитектура</h3>
<p>Архитектура сети получила название <em>Inception</em> (we need to go deeper).</p>
<p>Сверточные сети обычно имеют однотипную архитектуру: несколько сверток, <em>pooling</em>, полносвязные слои. Главное отличие <em>Inception</em> архитектуры в использовании новых <em>Inception</em> слоев.</p>
<p>Самый очевидный путь улучшения производительности нейронной сети — увеличение ее размера. Но больший размер сети, обычно, влечет увеличение числа параметров. Значит выше шанс оверфитнуться и ресурсов нужно больше. Эти проблемы можно решить используя разряженные слои вместо полносвязных. Но современное железо плохо справляется с разряженными структурами данных (кеш промахи).</p>
<p>Ключевая идея <em>Inception</em> — выяснить, как оптимальные локально-разряженные структуры сверточной сети могут быть аппроксимированы существующими неразряженными компонентами.</p>
<p><em>Inception</em> слой одновременно применяет к предыдущему слою 1x1, 3x3 и 5x5 свертки, 3x3 пулинг и комбинирует результаты. Это проще один раз <a href="https://www.youtube.com/watch?v=VxhSouuSZDY">увидеть</a>. Для уменьшения вычислительной сложности дополнительно используются 1x1 свертки.</p>
<p><img src="/img/imagenet/inceptionLayer.png" alt="Inception layer">
Схема <em>Inception</em> слоя.</p>
<h3 id="googlelenet">GoogleLeNet</h3>
<p><em>GoogleLeNet</em> является конкретной реализацией <em>Inception</em> архитектуры, использованной в соревновании <em>ILSVRC</em> 2014. Сеть содержит 22 слоя (27 если считать пулинг слои). На самом деле простых независимых слоев около 100.</p>
<p><img src="/img/imagenet/googleLeNet.png" alt="GoogleLeNet">
Схема <em>GoogleLeNet</em>.</p>
<p>Функция активации: <em>ReLU</em>. Обучали стохастическим градиентным спуском с импульсом. Для улучшения <em>backpropogation</em> дополнительно использовали вспомогательные классификаторы на основе простых сверточных сетей.</p>
<p>Обучили 7 одинаковых сетей, с разным семплингом и рандомизацией входных изображений. Объединили в ансамбль, получили state of the art.</p>
<ul>
<li>Ошибка на топ 5: 6.67%.</li>
</ul>
<p>Затем <em>GoogleLeNet</em> применили для детекции объектов, ансамбль из 6 сетей показал state of the art.</p>
</article>
<nav class="no-print post-nav">

	<a class="prev-post" href="re9ulus.github.io/post/2016-12-16-neural-network-resources/">
		<img class="icon-text" src="/img/prev.svg"/>Подборка материалов по нейронным сетям</a>


	<a class="next-post" href="re9ulus.github.io/post/2017-03-09-theano-win-install/">Ставим Theano на Windows<img class="icon-text" src="/img/next.svg"/>
	</a>

</nav>









<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMath();"></script>



<script>
function renderMath() {
  renderMathInElement(
      document.body,
      {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "\\[", right: "\\]", display: true},
              {left: "$", right: "$", display: false},
              {left: "\\(", right: "\\)", display: false}
          ]
      }
  );
}
</script>



<script type="text/javascript" >
   (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
   m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
   (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

   ym(29515615, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
   });
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/29515615" style="position:absolute; left:-9999px;" alt="" /></div></noscript>



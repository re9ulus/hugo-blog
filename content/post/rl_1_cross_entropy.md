---
author: re9ulus
date: 2020-01-01T00:00:00Z
draft: true
title: Заметки про RL, кросс-энтропия и посадка звездолетов
---

# RL

Обучение с подкреплением или `reinforcement learning` - раздел машинного обучения, изучающий действия `агента` в `среде` для достижения максимальной награды. В отличии от привычных методов обучения с учителем и обучения без учуителя у нас нет никаких данных. Вместо этого у нас есть `среда` и возможность попробовать какие-то действия. Как в классических русских сказках: "Налево пойдешь - коня потеряешь...".

`ТУТ НУЖНО ВСТАВИТЬ ЗАБАВНУЮ КАРТИНКУ С КАМНЕМ И ВЫБОРОМ ПУТИ`

Вот только даже камня с надписью у нас нет. Нужно самостоятельно попробовать походить во все стороны, потыкать все возможные ручки, понажимать все кнопки и узнать, как же выйти победителем из сложившейся ситуации.

Агент находится в нашем распоряжении...

Среду мы не контролируем, можем только получать информацию о текущем состоянии и решать что делать дальше. В зависимости от постановки задачи среда может быть
- Наблюдаемой (как в шахматах, мы видим всю доску)
- Частично наблюдаемой (`нужен пример`)
- Ненаблюдаемой (`нужен пример`)

`Тут нужно вставить картинку с сгентом и средой`

Агент изучает среду путем проб и ошибок.

Агент получает состояние среды `state` и решает что делать дальше `action`. Действие приводит к изменению состояния среды, например агент перемещается. Или падает в жерло горящего вулкана. За каждое действие агент получает награду `reward`. Задача агента - выработать стратегию `policy`, которая позволит ему действовать максимально выгодным образом и получить максимальную награду.

---
Диаграмма со средой, агентом action/observation

Агент в нашем распоряжении
Среда в худшем случае - черная коробка

ключевые идеи:
- возможность самому выбирать действия и исследовать среду
- недифференцируемая награда, которая происходит в следствии общения со средой (лайки в соцсети)
---

# Процесс принятия решений

### MDP process

Взаимодействие агента со средой можно представить в виде марковского процесса принятия решений (marov decision process) или `MDP`.

Ключевое свойство марковского процесса - отсутствие памяти. То есть текущее состояние зависит только от предыдущего и не зависит от остальной истории.

$$P(s\_{t+1} | s\_t, a\_t, s\_{t-1}, a\_{t-1}) = P(s\_{t+1} | s\_t, a\_t)$$

- Состояния среды (States): $s \in S$
- Действия агента (Actions): $a \in A$
- Награды (Rewards): $r \in R$

### Награды

Награда за сессию:

$$R = \sum r\_t$$

Agents policy (политика, стратегия) - определяет вероятностное распределение

$$\pi (a | s) = P(take\ action\ a | in\ state\ s)$$

Задача: найти стратегию ведущую к максимальной награде: $\pi (a | s) : E\_{\pi}[R] -> max$

### Метод кросс-энтропии

Общая идея:
- Играем несколько сессий
- Выбираем элетные сессии `elite` с лучшими `R`
- Обновляем стратегию используя `elite`
- Повтяем

# Метод можно отнести к DFO (Derivative-free-optimization) / Evolution optimization
[Видео](https://www.youtube.com/watch?v=aUrX-rP_ss4&list=PLCTc_C7itk-GaAMxmlChrkPnGKtjz8hv1)

In each episode initial state is sampled from \mu and the process proceed until the terminal state is reached. For example:
- Taxi robot reaches it's destination (termination = good)
- Walking robot falls over (termniation = bad)

Types of parametrized policies:
- Deterministic $a = \pi(s, theta)$
- Stochastic $a = \pi(a|s, theta)$

- Parameterized $policies \pi\_{\Theta}$

Objective: maximize $\nu(\pi) = E [r\_0, .. r\_{t-1} | \pi]$

### Табличный метод кросс-энтропии (Tabular crossentropy method)

- Policy matrix
$$\pi(a | s) = A\_{s, a}$$
- Sample N games with that policy
- Get M best sessions (elites)
$$Elite = [(s\_0, a\_0), (s\_1, a\_1) ... (s\_k, a\_k)]$$
- Aggregate by states
$$\pi (a | s) = (took\ a\ at\ s) / (was\ at\ s)$$  <- In M best games

- Алгоритм очень нестабилен при малых выборках. Пусть в одном из состояний мы оказались 1 раз. Тогда всегда попадая в это состояние мы будем идти в одну сторону.
Для починки можно использовать сглаживание, экспоненциальное скользящее среднее или другие костыли;
- Если в среде есть элемент случайности - алгоритм отберет в качестве лучших те исполнения, где ему повезло. Алгоритм никогда не научиться правильно обрабатывать неудачные для него среды;
- Награду мы получаем в конце сессии, значит чтобы хоть чему-то научиться нужно закончить игру. Другие методы позволяют аненту учиться по ходу игровой сессии, это может быть важно при длинных или бесконечных сессиях;
- Нужно много итераций.

А если таблица слишком большая и не лезет в память... просто построим распределение на действия использя стандартные алгоритмы машинного обучения.

### Approxymate CrossEntropy Method

Теперь policy это аппроксимация нашей таблицы.

- Policy is approxymation
  - Neural network predicts $\pi\_w (a | s)$ given s
  - Linear model, Random forest, etc.

Can't set $\pi(a | s)$ explicitly

All state-action pairs from M best sessions

$$Elite = [(s\_0, a\_0), (s\_1, a\_1), ... (s\_k, a\_k)]$$

Maximize likelihood of actions in "best" games

$$\pi = argmax\_{\pi} \sum\_{a, s \in Elite} log \pi(a\_i, s\_i)$$

Algorithm:
- Initialize NN weights $W\_0$ <- random
- Loop:
  - Sample N Sessions
  - $Elite = [(s\_0, a\_0), (s\_1, a\_1), ... (s\_k, a\_k)]$
  - $W\_{i+1} = W\_i + alpha \grad [\sum\_{a\_i, s\_i} \in Elite} log \pi\_{w\_i} (a\_i | s\_i)]


# Методы монте-карло
[Видео](https://yadi.sk/i/5yf_4oGI3EDJhJ)

Crossentropy Method
- Stochastic optimization

Monte Carlo Policy Gradient

###
Код с обучением и посадкой звездолета
###


Источники:
- [Курс ШАД по RL](https://github.com/yandexdataschool/Practical_RL)

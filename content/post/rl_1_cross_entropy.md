---
author: re9ulus
date: 2020-01-01T00:00:00Z
draft: true
title: Заметки про RL, кросс-энтропия и посадка звездолетов
---

# RL

Обучение с подкреплением или `reinforcement learning` - раздел машинного обучения, изучающий действия `агента` в `среде` для достижения максимальной награды. В отличии от привычных методов обучения с учителем и обучения без учуителя у нас нет никаких данных. Вместо этого у нас есть `среда` и возможность попробовать какие-то действия. Как в классических русских сказках: "Налево пойдешь - коня потеряешь...".

`ТУТ НУЖНО ВСТАВИТЬ ЗАБАВНУЮ КАРТИНКУ С КАМНЕМ И ВЫБОРОМ ПУТИ`

Вот только даже камня с надписью у нас нет. Нужно самостоятельно попробовать походить во все стороны, потыкать все возможные ручки, понажимать все кнопки и узнать, как же выйти победителем из сложившейся ситуации.

Агент находится в нашем распоряжении...

Среду мы не контролируем, можем только получать информацию о текущем состоянии и решать что делать дальше. В зависимости от постановки задачи среда может быть
- Наблюдаемой (как в шахматах, мы видим всю доску)
- Частично наблюдаемой (`нужен пример`)
- Ненаблюдаемой (`нужен пример`)

`Тут нужно вставить картинку с сгентом и средой`

Агент изучает среду путем проб и ошибок.

Агент наблюдает состояние среды $s$ и в соответствии со своей политикой $\pi$ выбирает действие $a$. Действие приводит среду в новое состояние$s'$, например агент перемещается. Кроме того, за действие агент получает награду $r$. Задача агента - найти стратегию $pi$, которая поможет еему получать максимальную награду за игровую сессию.

---
Диаграмма со средой, агентом action/observation

Агент в нашем распоряжении
Среда в худшем случае - черная коробка

ключевые идеи:
- возможность самому выбирать действия и исследовать среду
- недифференцируемая награда, которая происходит в следствии общения со средой (лайки в соцсети)
---

### Постановка задачи

Взаимодействие агента со средой можно представить в виде марковского процесса принятия решений (Markov decision process) или `MDP`.

Ключевое свойство марковского процесса - отсутствие памяти. Текущее состояние зависит только от предыдущего и не зависит от остальной истории.

$$P(s\_{t+1} | s\_t, a\_t, s\_{t-1}, a\_{t-1}) = P(s\_{t+1} | s\_t, a\_t)$$

Где
- $s \in S$ состояние среды `state`
- $a \in A$ действие `action`
- $r \in R$ награда `reward`

агент последовательно совершает действия и накапливает награду за сессию

$$R = \sum r\_t$$

Политика `policy` (или иначе - стратегия) агента определяет вероятностное распределение на действия в завимости от состояния среды.

$$\pi (a | s) = P(выбрать\ действие\ a | в\ состоянии\ s)$$

Задача: найти стратегию ведущую к максимальной награде: $\pi (a | s) : E\_{\pi}[R] -> max$ То есть в каждом состоянии мы должны уметь выбирать действие, которое где-то там, в будущем поможет агенту набрать максимальную награду.

### Метод кросс-энтропии

Общая идея метода:
1. Играем несколько сессий. Тут все очевидно, сначала нам нужно попробовать разные действия в разных состояниях и посмотреть что получается.

2. Выбирем среди сессий элитные `elite`, такие у которы суммарная награда $\sum r$ максимальна. Можно предположить, что в элитных сессиях агент действовал чуть-более оптимально, поэтому давайти перенастроим агента чаще выбирать действия, которые он выбирал в элитных сессиях.

3. Обновляем стратегию `\pi`. К этому мы перейдем дальше.

4. Возвращаемся к шагу 1 и повторяем

# Метод можно отнести к DFO (Derivative-free-optimization) / Evolution optimization
[Видео](https://www.youtube.com/watch?v=aUrX-rP_ss4&list=PLCTc_C7itk-GaAMxmlChrkPnGKtjz8hv1)

In each episode initial state is sampled from \mu and the process proceed until the terminal state is reached. For example:
- Taxi robot reaches it's destination (termination = good)
- Walking robot falls over (termniation = bad)

Types of parametrized policies:
- Deterministic $a = \pi(s, theta)$
- Stochastic $a = \pi(a|s, theta)$

- Parameterized $policies \pi\_{\Theta}$

Objective: maximize $\nu(\pi) = E [r\_0, .. r\_{t-1} | \pi]$

### Табличный метод кросс-энтропии (Tabular crossentropy method)

В простейшем случае политику можно задать таблицей. По строкам у нас будут состояния, а по колонкам действия. Тогда каждое число в таблице это вероятность совершить действие $j$ оказавшись в состоянии $i$.

Алгоритма работы с таблицей:
1. Инициализируем политику (таблицу) $$\pi(a | s) = A\_{s, a}$$ равномерным распределением $1 / |A|$
2. Играем $N$ игр руководствуясь политикой $\pi$
3. Отбираем $M$ лучших сессий $Elite = [(s\_0, a\_0), (s\_1, a\_1) ... (s\_k, a\_k)]$ с максимальными наградами $\sum R$
4. Обновляем $\pi$ в соответствии с лучшими сессиями $\pi (a | s) = (сделали a\ при\ s) / (при\ s), for M$
5. Переходим к шагу 2

Если таблица слишком большая и не лезет в память... просто построим распределение с помощью стандартных алгоритмов машинного обучения.

### Approxymate CrossEntropy Method

Аппроксимируем нашу таблицу. Например нейронная сеть будет описывать политику $\pi\_w (a | s)$, если принимает на вход состояние $s$ и возвращает вероятности действий. Подойдет любой алгоритм возврашающий вероятности: линейные модели, случайные леса, etc.

Алгоритм:
1. Инициализируем случайную политику (случайные веса нейронной сети)
2. Играем $N$ игр руководствуясь политикой $\pi$
3. Отбираем $M$ лучших сессий $Elite = [(s\_0, a\_0), (s\_1, a\_1) ... (s\_k, a\_k)]$ с максимальными наградами $\sum R$
4. Обновляем политику в соответствии с лучшими сессиями (дообучаем нейронную сеть на примерах из элитных сессий) $W\_{i+1} = W\_i + alpha \grad [\sum\_{a\_i, s\_i} \in Elite} log \pi\_{w\_i} (a\_i | s\_i)]
5. Переходим к шагу 2

### Недостатки метода

- Алгоритм очень нестабилен при малых выборках. Пусть в одном из состояний мы оказались 1 раз. Тогда всегда попадая в это состояние мы будем идти в одну сторону.
Для починки можно использовать сглаживание, экспоненциальное скользящее среднее или другие костыли;
- Если в среде есть элемент случайности - алгоритм отберет в качестве лучших те исполнения, где ему повезло. Алгоритм никогда не научиться правильно обрабатывать неудачные для него среды;
- Награду мы получаем в конце сессии, значит чтобы хоть чему-то научиться нужно закончить игру. Другие методы позволяют аненту учиться по ходу игровой сессии, это может быть важно при длинных или бесконечных сессиях;
- Нужно много итераций.


# Методы монте-карло
[Видео](https://yadi.sk/i/5yf_4oGI3EDJhJ)

Crossentropy Method
- Stochastic optimization

Monte Carlo Policy Gradient

###
Код с обучением и посадкой звездолета
###


Источники:
- [Курс ШАД по RL](https://github.com/yandexdataschool/Practical_RL)
